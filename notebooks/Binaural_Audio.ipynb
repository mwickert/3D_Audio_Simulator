{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binaural Audio Processing Using CIPIC and CIPIC-Like Spherical Head HRIR Data Sets\n",
    "This notebook contains primarily two 3D audio apps:\n",
    "\n",
    "1. A static sound source positioned in real-time using Jupyter widget sliders\n",
    "2. A dynamic sound source flying a *trajectory* with parameters controlled via Jupyter widget sliders\n",
    "\n",
    "In both cases the basic 3D audio simulation, which makes of `pyaudio_helper` from `Scikit-DSP-Comm` takes the genral form shown below:\n",
    "\n",
    "\n",
    "<img src=\"figures/3D_Audio_app_general_block_diagram.png\" width=\"75%\">\n",
    "<!--- Image('figures/3D_Audio_app_general_block_diagram.png',width='90%')--->\n",
    "\n",
    "A simplified block diagram of PyAudio *streaming-based* (nonblocking) signal processing is shown below. The package `pyaudio` needs to be installed as the underpining to `Scikit-DSP-Comm`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/pyaudio_dsp_IO.png\" width=\"75%\">\n",
    "<!--- Image('figures/pyaudio_dsp_IO.png',width='90%')--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sk_dsp_comm.sigsys as ss\n",
    "import sk_dsp_comm.pyaudio_helper as pah\n",
    "import sk_dsp_comm.fir_design_helper as fir_d\n",
    "import scipy.signal as signal\n",
    "import scipy.io as io\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import Audio, display\n",
    "from IPython.display import Image, SVG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure Rendering Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylab.rcParams['savefig.dpi'] = 100 # default 72\n",
    "#pylab.rcParams['figure.figsize'] = (6.0, 4.0) # default (6,4)\n",
    "#%config InlineBackend.figure_formats=['png'] # default for inline viewing\n",
    "%config InlineBackend.figure_formats=['svg'] # SVG inline viewing\n",
    "#%config InlineBackend.figure_formats=['pdf'] # render pdf figs for LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'name': 'Built-in Microphone', 'inputs': 2, 'outputs': 0},\n",
       " 1: {'name': 'Built-in Output', 'inputs': 0, 'outputs': 2}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pah.available_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Real-Time Mono Playback Test App\n",
    "The case of real-time playback sends an `ndarray` through the chosen audio output path with the array data either being truncated or looped depending upon the length of the array relative to `Tsec` supplied to `stream(Tsec)`. To manage the potential looping aspect of the input array, we first make a `loop_audio` object from the input array. An example of this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callback (2)\n",
    "# Here we configure the callback to play back a wav file                      \n",
    "def callback0(in_data, frame_count, time_info, status):\n",
    "    global DSP_IO, x\n",
    "    DSP_IO.DSP_callback_tic()\n",
    "    \n",
    "    # Ignore in_data when generating output only\n",
    "    #***********************************************\n",
    "    # Note wav is scaled to [-1,1] so need to rescale to int16\n",
    "    y = 32767*x.get_samples(frame_count)\n",
    "    # Perform real-time DSP here if desired\n",
    "    #\n",
    "    #***********************************************\n",
    "    # Save data for later analysis\n",
    "    # accumulate a new frame of samples\n",
    "    DSP_IO.DSP_capture_add_samples(y)\n",
    "    #***********************************************\n",
    "    # Convert from float back to int16\n",
    "    y = y.astype(int16)\n",
    "    DSP_IO.DSP_callback_toc()\n",
    "    return y.tobytes(), pah.pyaudio.paContinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ccd3b7c6eb44a448d0ffecae226fa61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description=' ', index=1, options=('Start Streaming', 'Stop Streaming'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fs, x_wav2 = ss.from_wav('Music_Test.wav')\n",
    "x_wav = (x_wav2[:,0] + x_wav2[:,1])/2 #combine the left and right channels\n",
    "x = pah.loop_audio(x_wav)\n",
    "DSP_IO = pah.DSP_io_stream(callback0,0,1,fs=44100,Tcapture=0)\n",
    "DSP_IO.interactive_stream(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to the CIPIC Interaural Polar Coordinates\n",
    "\n",
    "CIPIC uses a special *interaural polar coordinate system* (IPCS) that needs to be addressed in order to make a 3D audio demo. Two other aspects to be consider are:\n",
    "\n",
    "1. CIPIC assumes the sound source lies on a sphere of radius 1m, so due to sound wave divergence, the amplitude needs to be scaled inversely with radial distance (inverse-squared in the sound intensity sense).\n",
    "2. To properly represent a sound source closer than 1m there is a parallax error that must be dealt with as explained in [Fitzpatrick].\n",
    "\n",
    "The ultimate goal is to represent an audio source arriving from any set of coordinates, in this case $(x_1,y_1,z_1$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ss_mapping2CIPIChrir(object):\n",
    "    \"\"\"\n",
    "    A class for sound source mapping to the CIPIC HRIR database\n",
    "    \n",
    "    CIPIC uses the interaural polar coordinate system (IPCS).\n",
    "    The reference sphere for the head-related transfer function \n",
    "    (HRTF) measurements/head-related impulse response (HRIR) \n",
    "    measurements has a 1m radius.\n",
    "    \n",
    "    \n",
    "    Mark Wickert June 2018\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,sub_foldername,head_radius_cm = 8):\n",
    "        \"\"\"\n",
    "        Object instantiation\n",
    "        \n",
    "        The default head radius is 8cm or 0.08m\n",
    "        \"\"\"\n",
    "        # Store the head radius in meters\n",
    "        self.head_radius = head_radius_cm/100\n",
    "        \n",
    "        # Store the HRIR 200 tap FIR filter coefficient sets\n",
    "        self.subject = sub_foldername\n",
    "        hrir_LR = io.loadmat( self.subject + '/hrir_final.mat')\n",
    "        self.hrirL = hrir_LR['hrir_l']\n",
    "        self.hrirR = hrir_LR['hrir_r']\n",
    "        \n",
    "        # Create LUTs for the azimuth and elevation \n",
    "        # values. This will make it easy to quantize\n",
    "        # a given source location to one of the \n",
    "        # available HRIRs in the database.\n",
    "        self.Az_LUT = hstack(([-80,-65,-55],\n",
    "                      arange(-45,45+5,5.0),[55,65,80]))\n",
    "        self.El_LUT = -45 + 5.625*arange(0,50)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.tR = 1 # place source on unit sphere\n",
    "        self.tL = 1 # directly in front of listener\n",
    "        self.elRL = 0\n",
    "        self.azR = 0\n",
    "        self.azL = 0\n",
    "        self.AzR_idx = 0\n",
    "        self.AzL_idx = 0\n",
    "        self.ElRL_idx = 0\n",
    "        \n",
    "        # Store corresponding right and left ear FIR \n",
    "        # filter coefficients\n",
    "        self.coeffR = self.hrirR[0,0,:]\n",
    "        self.coeffL = self.hrirL[0,0,:]\n",
    "        \n",
    "    \n",
    "    def cart2ipcs(self,x,y,z):\n",
    "        \"\"\"\n",
    "        Map cartesian source coordinates (x,y,z) to the \n",
    "        CIPIC interaural polar coordinate system (IPCS) \n",
    "        for easy access to CIPIC HRIR. Parallax error is \n",
    "        also dealt with so two azimuth values are found. \n",
    "        To fit IPCS the cartesian coordinates are defined \n",
    "        as follows:\n",
    "\n",
    "        (0,0,0) <--> center of head.\n",
    "        (1,0,0) <--> unit vector pointing outward from the right \n",
    "                     on a line passing from left to right through\n",
    "                     the left and right ear (pinna) ear canals\n",
    "        (0,1,0) <--> unit vector pointing out through the top \n",
    "                     of the head.\n",
    "        (0,0,1) <--> unit vector straight out through the back of \n",
    "                     the head, such that a right-handed coordinate\n",
    "                     system is formed.\n",
    "\n",
    "        Mark Wickert June 2018\n",
    "        \"\"\"\n",
    "        # First solve for the parameter t, which is used to describe \n",
    "        # parametrically the location of the source at (x1,y1,z1) on a line \n",
    "        # connecting the right or left ear canal entry point to the \n",
    "        # unit sphere.\n",
    "\n",
    "        # The right ear (pinna) solution\n",
    "        aR = (x-self.head_radius)**2 + y**2 + z**2\n",
    "        bR = 2*self.head_radius*(x-self.head_radius)\n",
    "        cRL = self.head_radius**2 - 1\n",
    "        # The left ear (pinna) solution\n",
    "        aL = (x+self.head_radius)**2 + y**2 + z**2\n",
    "        bL = -2*self.head_radius*(x+self.head_radius)\n",
    "\n",
    "        # Find the t values which are also the gain values \n",
    "        # to be applied to the filter.\n",
    "        self.tR = max((-bR+sqrt(bR**2-4*aR*cRL))/(2*aR),\n",
    "                 (-bR-sqrt(bR**2-4*aR*cRL))/(2*aR))\n",
    "        self.tL = max((-bL+sqrt(bL**2-4*aL*cRL))/(2*aL),\n",
    "                 (-bL-sqrt(bL**2-4*aL*cRL))/(2*aL))\n",
    "        #print('tR = %6.2e, tL = %6.2e' % (self.tR,self.tL))\n",
    "        elRL = 180/pi*arctan2(y,-z)\n",
    "        if elRL < -90:\n",
    "            elRL += 360\n",
    "        self.elRL = elRL\n",
    "        self.azR = 180/pi*arcsin(clip(self.head_radius \\\n",
    "                                 + self.tR*(x-self.head_radius),-1,1))\n",
    "        self.azL = 180/pi*arcsin(clip(-self.head_radius \\\n",
    "                                 + self.tL*(x+self.head_radius),-1,1))\n",
    "        #print('elRL = %4.2f, azR = %4.2f, azL = %4.2f' \\\n",
    "        #      % (self.elRL,self.azR,self.azL))\n",
    "        \n",
    "        self.AzR_idx = argmin((self.Az_LUT - self.azR)**2)\n",
    "        self.AzL_idx = argmin((self.Az_LUT - self.azL)**2)\n",
    "        self.ElRL_idx = argmin((self.El_LUT - self.elRL)**2)\n",
    "        self.coeffR = self.hrirR[self.AzR_idx,self.ElRL_idx,:]\n",
    "        self.coeffL = self.hrirL[self.AzL_idx,self.ElRL_idx,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlay Plot of the Right and Left Ear Impulse Responses versus Source Location\n",
    "The next code cell creates an interactive plot of the right and left channel HRIR as a function of the source location in the cylindical coordinates shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/3D_Coordinates.png\" width=\"75%\">\n",
    "<!--- Image('figures/3D_Coordinates.png',width='90%')--->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this repository two subject HRIR data sets are provided, `subject_200` and `subject_201`. These subjects correspond to simple spherical head models. The explanation of these data sets can be found in the notebook: `CIPIC_read_write.ipynb`. To explore ture human subject HRIR data sets you need to visit the CIPIC site: https://www.ece.ucdavis.edu/cipic/spatial-sound/hrtf-data/. In particular at from this site you can download a [ZIP](https://ucdavis.app.box.com/s/wrxylwv65q4ll69xri89pduhd4w4coqr) file that contains data sets for 45 subjects that were obtaine in an anechoic chamber. A good starting point is the folder `subject_165`, for the manikin KEMAR (https://www.gras.dk/industries/audiology/kemar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e3eaf2a0a9641e18c76ae3fccfcfed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=90, description='p_theta', max=360, step=5), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject = ss_mapping2CIPIChrir('subject_200') # subject_165 or subject_200\n",
    "@interact(p_theta = widgets.IntSlider(min=0,max=360,step=5,value=90),\n",
    "          p_r = widgets.FloatSlider(min=0.1,max=4,step=0.1,value=1),\n",
    "          p_y = widgets.FloatSlider(min=-5,max=5,step=0.1,value=0.0))\n",
    "def f(p_theta,p_r,p_y):\n",
    "    subject.cart2ipcs(p_r*sin(pi/180*p_theta),\n",
    "                      p_y,\n",
    "                      p_r*cos(pi/180*p_theta))\n",
    "    \n",
    "    t = arange(0,200)/44.100\n",
    "    plot(subject.tR*subject.coeffR) # /1.5\n",
    "    plot(subject.tL*subject.coeffL) # /1.5\n",
    "    title(r'Head-Related Impulse Response versus Source Location')\n",
    "    ylabel(r'Amplitude')\n",
    "    xlabel(r'Time (ms)')\n",
    "    legend((r'Right',r'Left'))\n",
    "    ylim([-1.4,1.4])\n",
    "    grid();\n",
    "#     savefig('hrir_130_R875.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_xz_plane = widgets.FloatSlider(description = 'r_xz (m)', \n",
    "                continuous_update = True,\n",
    "                value = 1.0, # At one meter away\n",
    "                min = 0.2, \n",
    "                max = 3.0, \n",
    "                step = 0.05, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "azimuth = widgets.FloatSlider(description = 'az (deg)', \n",
    "                continuous_update = True,\n",
    "                value = 90.0, # In front\n",
    "                min = 0.0, \n",
    "                max = 360, \n",
    "                step = 5, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "y_axis = widgets.FloatSlider(description = 'h_y (m)', \n",
    "                continuous_update = True,\n",
    "                value = 0.0, # Level with ears (pinna)\n",
    "                min = -5.0, \n",
    "                max = 5.0, \n",
    "                step = 0.05, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "Gain = widgets.FloatSlider(description = 'Gain', \n",
    "                continuous_update = True,\n",
    "                value = 0.2,\n",
    "                min = 0.0, \n",
    "                max = 2.0, \n",
    "                step = 0.01, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "#widgets.HBox([Gain,r_xz_plane,azimuth,y_axis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback(in_data, frame_length, time_info, status):\n",
    "    global DSP_IO,subject,zi_left,zi_right,r_xz_plane\n",
    "    global azimuth,y_axis,Gain,x_loop\n",
    "    DSP_IO.DSP_callback_tic()\n",
    "    # convert byte data to ndarray\n",
    "    in_data_nda = np.frombuffer(in_data, dtype=np.int16)\n",
    "    # separate left and right data\n",
    "    x_left,x_right = DSP_IO.get_LR(in_data_nda.astype(float32))    \n",
    "    #***********************************************\n",
    "    # DSP operations here:\n",
    "    # Apply Kemar HRIR left and right channel filters at\n",
    "    # the sound source location in cylindrical coordinates\n",
    "    # mapped to cartesian coordinates\n",
    "    # The input to both filters comes by first \n",
    "    # combining x_left & x_right channels\n",
    "#     x_mono = Gain.value*(x_left + x_right)/2\n",
    "#     x_mono = Gain.value*20000*x_loop.get_samples(frame_length)\n",
    "    x_mono = Gain.value*5000*randn(frame_length) #input white noise\n",
    "    subject.cart2ipcs(r_xz_plane.value*sin(pi/180*azimuth.value), #x\n",
    "                      y_axis.value, #y\n",
    "                      r_xz_plane.value*cos(pi/180*azimuth.value)) #z \n",
    "    y_left, zi_left = signal.lfilter(subject.coeffL,1,\n",
    "                                     subject.tL*x_mono,zi=zi_left) \n",
    "    y_right, zi_right = signal.lfilter(subject.coeffR,1,\n",
    "                                       subject.tR*x_mono,zi=zi_right)\n",
    "    #***********************************************\n",
    "    # Pack left and right data together\n",
    "    y = DSP_IO.pack_LR(y_left,y_right)   \n",
    "    #***********************************************\n",
    "    # Save data for later analysis\n",
    "    # accumulate a new frame of samples\n",
    "    DSP_IO.DSP_capture_add_samples_stereo(y_left,y_right)\n",
    "    #***********************************************\n",
    "    # Convert from float back to int16\n",
    "    y = y.astype(int16)\n",
    "    DSP_IO.DSP_callback_toc()\n",
    "    # Convert ndarray back to bytes\n",
    "    return y.tobytes(), pah.pyaudio.paContinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d39dc4e3804b6ba71ea8e56041a484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description=' ', index=1, options=('Start Streaming', 'Stop Streaming'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce11482bfd3452f98cd3d598cbbd309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.2, description='Gain', max=2.0, orientation='vertical', step=0.01), FloatSl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a ss_mapping2CIPIChrir object\n",
    "# SUBJECT 20, 21 (KEMAR sm), & 165 (KEMAR LG) available now\n",
    "# subject_200, 201 is 8.75 cm, 10 cm sphere\n",
    "subject = ss_mapping2CIPIChrir('subject_200')\n",
    "# Initialize L/R filter initial conditions\n",
    "zi_left = signal.lfiltic(subject.coeffL,1,[0])\n",
    "zi_right = signal.lfiltic(subject.coeffR,1,[0])\n",
    "# Load loop audio as a single channel/mono source\n",
    "fs, x_wav_mt = ss.from_wav('Music_Test.wav')\n",
    "x_wav_mt = (x_wav_mt[:,0] + x_wav_mt[:,1])/2\n",
    "x_loop = pah.loop_audio(x_wav_mt)\n",
    "# Create a IO stream object and start streaming\n",
    "DSP_IO = pah.DSP_io_stream(callback,0,1,frame_length=1024, \n",
    "                           fs=44100,Tcapture=0)\n",
    "DSP_IO.interactive_stream(0,2)\n",
    "widgets.HBox([Gain,r_xz_plane,azimuth,y_axis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving the Sound Source Over a Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snd_orbit(phi,r_xy,theta_roll,theta_pitch,h_y):\n",
    "    \"\"\"\n",
    "    xrp,yrp,zrp = snd_orbit(T0,r_xy,theta_roll,theta_yaw)\n",
    "    \n",
    "    Mark Wickert June 2018\n",
    "    \"\"\"\n",
    "    # Increment phi using a state variable\n",
    "    z = r_xy*cos(phi*pi/180)\n",
    "    x = r_xy*sin(phi*pi/180)\n",
    "    if type(phi) == numpy.ndarray:\n",
    "        y = zeros(len(phi))\n",
    "    else:\n",
    "        y = 0\n",
    "\n",
    "    # Roll: x-y plane rotation\n",
    "    zr = z\n",
    "    xr = x*cos(theta_roll*pi/180) - y*sin(theta_roll*pi/180)\n",
    "    yr = x*sin(theta_roll*pi/180) + y*cos(theta_roll*pi/180)\n",
    "\n",
    "    # Pitch: y-z plane rotation\n",
    "    zrp = yr*sin(theta_pitch*pi/180) + zr*cos(theta_pitch*pi/180)\n",
    "    xrp = xr\n",
    "    yrp = yr*cos(theta_pitch*pi/180) - zr*sin(theta_pitch*pi/180)\n",
    "    yrp = yrp + h_y\n",
    "    return xrp, yrp, zrp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84786734fba42c3b50e87bfb5acd5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='r_xz', max=3.0, min=0.1), FloatSlider(value=0.0, des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(r_xz = widgets.FloatSlider(min=0.1,max=3,step=.1,value=1),\n",
    "          theta_roll = widgets.FloatSlider(min=-80,max=80,step=5.0,value=0),\n",
    "          theta_pitch = widgets.FloatSlider(min=-80,max=80,step=5.0,value=0),\n",
    "          h_y = widgets.FloatSlider(min=-3,max=3,step=.2,value=0))\n",
    "def traj_plot(r_xz,theta_roll,theta_pitch,h_y):\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    #ax.set_aspect('equal')\n",
    "    \n",
    "    u, v = np.mgrid[0:2*np.pi:20j, 0:np.pi:10j]\n",
    "    x = .12*np.cos(u)*np.sin(v)\n",
    "    y = .12*np.sin(u)*np.sin(v)\n",
    "    z = .12*np.cos(v)\n",
    "    ax.plot_wireframe(x, y, z, color=\"r\")\n",
    "    \n",
    "    phi = arange(0,361,1)\n",
    "    xrp, yrp, zrp = snd_orbit(phi,r_xz,theta_roll,theta_pitch,h_y)\n",
    "    ax.plot(zrp,xrp,yrp,'b',linewidth=1.0)\n",
    "\n",
    "    ax.set_xlim3d(-1.2,1.2)\n",
    "    ax.set_ylim3d(-1.2,1.2)\n",
    "    ax.set_zlim3d(-1.2,1.2)\n",
    "    ax.set_xlabel(r'$z$ (m)')\n",
    "    ax.set_ylabel(r'$x$ (m)')\n",
    "    ax.set_zlabel(r'$y$ (m)')\n",
    "    ax.set_title(r'Sound Source Trajectory (CCW)')\n",
    "    ax.set_aspect('equal')\n",
    "    #axis('scaled')\n",
    "    ax.view_init(elev = 20, azim = 45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_xz_T = widgets.FloatSlider(description = 'r_xz (m)', \n",
    "                continuous_update = True,\n",
    "                value = 1.0, # At one meter away\n",
    "                min = 0.2, \n",
    "                max = 3.0, \n",
    "                step = 0.05, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "theta_roll_T = widgets.FloatSlider(description = 'roll (deg)', \n",
    "                continuous_update = True,\n",
    "                value = 0.0, # In front\n",
    "                min = -80.0, \n",
    "                max = 80.0, \n",
    "                step = 5, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "theta_pitch_T = widgets.FloatSlider(description = 'pitch (deg)', \n",
    "                continuous_update = True,\n",
    "                value = 0.0, # In front\n",
    "                min = -80.0, \n",
    "                max = 80.0, \n",
    "                step = 5, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "h_y_T = widgets.FloatSlider(description = 'h_y (m)', \n",
    "                continuous_update = True,\n",
    "                value = 0.0, # Level with ears (pinna)\n",
    "                min = -5.0, \n",
    "                max = 5.0, \n",
    "                step = 0.05, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "Gain_T = widgets.FloatSlider(description = 'Gain', \n",
    "                continuous_update = True,\n",
    "                value = 0.2,\n",
    "                min = 0.0, \n",
    "                max = 2.0, \n",
    "                step = 0.01, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "Period_T = widgets.FloatSlider(description = 'Period (s)', \n",
    "                continuous_update = True,\n",
    "                value = 5,\n",
    "                min = 0.1, \n",
    "                max = 10, \n",
    "                step = 0.1, \n",
    "                orientation = 'vertical')\n",
    "\n",
    "phi_T = 0\n",
    "#widgets.HBox([Gain_T,Period_T,r_xz_T,theta_roll_T,theta_pitch_T,h_y_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbackTraj(in_data, frame_length, time_info, status):\n",
    "    global DSP_IO,subject,zi_left,zi_right, phi_T, x_loop\n",
    "    global Gain_T,Period_T,r_xz_T,theta_roll_T,theta_pitch_T,h_y_T\n",
    "    DSP_IO.DSP_callback_tic()\n",
    "    # convert byte data to ndarray\n",
    "    in_data_nda = np.frombuffer(in_data, dtype=np.int16)\n",
    "    # separate left and right data\n",
    "    x_left,x_right = DSP_IO.get_LR(in_data_nda.astype(float32))    \n",
    "    #***********************************************\n",
    "    # Trajectory phase (rad)\n",
    "    Dphi = 1/Period_T.value*2*pi*frame_length/44100\n",
    "    # DSP operations here:\n",
    "    # Apply Kemar HRIR left and right channel filters at\n",
    "    # the sound source location in cylindrical coordinates\n",
    "    # mapped to cartesian coordinates\n",
    "    # The input to both filters comes by first \n",
    "    # combining x_left & x_right channels\n",
    "#     x_mono = Gain_T.value*(x_left + x_right)/2\n",
    "    x_mono = Gain_T.value*20000*x_loop.get_samples(frame_length)\n",
    "#     x_mono = Gain_T.value*5000*randn(frame_length) #input white noise\n",
    "    x, y, z = snd_orbit(phi_T*180/pi,r_xz_T.value,theta_roll_T.value,\n",
    "                        theta_pitch_T.value,h_y_T.value)\n",
    "    subject.cart2ipcs(x,y,z)\n",
    "#     subject.cart2ipcs(0,0,-1)\n",
    "    y_left, zi_left = signal.lfilter(subject.coeffL,1,\n",
    "                                     subject.tL*x_mono,zi=zi_left) \n",
    "    y_right, zi_right = signal.lfilter(subject.coeffR,1,\n",
    "                                       subject.tR*x_mono,zi=zi_right)\n",
    "    #***********************************************\n",
    "    # Pack left and right data together\n",
    "    y = DSP_IO.pack_LR(y_left,y_right)\n",
    "    # Typically more DSP code here     \n",
    "    #***********************************************\n",
    "    # Save data for later analysis\n",
    "    # accumulate a new frame of samples\n",
    "    DSP_IO.DSP_capture_add_samples_stereo(y_left,y_right)\n",
    "    #***********************************************\n",
    "    phi_T = mod(phi_T+Dphi,2*pi)\n",
    "    # Convert from float back to int16\n",
    "    y = y.astype(int16)\n",
    "    DSP_IO.DSP_callback_toc()\n",
    "    # Convert ndarray back to bytes\n",
    "    return y.tobytes(), pah.pyaudio.paContinue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a820a98264429cbc185345c9e2464f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description=' ', index=1, options=('Start Streaming', 'Stop Streaming'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8640273ba00451887ac6b6d445e9e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatSlider(value=0.22, description='Gain', max=2.0, orientation='vertical', step=0.01), FloatS…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a ss_mapping2CIPIChrir object\n",
    "# SUBJECT 20, 21 (KEMAR sm), & 165 (KEMAR LG) available now\n",
    "subject = ss_mapping2CIPIChrir('subject_200')\n",
    "# Initialize L/R filter initial conditions\n",
    "zi_left = signal.lfiltic(subject.coeffL,1,[0])\n",
    "zi_right = signal.lfiltic(subject.coeffR,1,[0])\n",
    "# Load loop audio\n",
    "fs, x_wav_mt = ss.from_wav('Music_Test.wav')\n",
    "x_wav_mt = (x_wav_mt[:,0] + x_wav_mt[:,1])/2\n",
    "x_loop = pah.loop_audio(x_wav_mt)\n",
    "# Create a IO stream object and start streaming\n",
    "DSP_IO = pah.DSP_io_stream(callbackTraj,0,1,frame_length=1024, \n",
    "                           fs=44100,Tcapture=0)\n",
    "DSP_IO.interactive_stream(0,2)\n",
    "widgets.HBox([Gain_T,Period_T,r_xz_T,theta_roll_T,theta_pitch_T,h_y_T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
